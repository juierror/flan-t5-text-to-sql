{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e97815e-9ab4-4d96-873d-4638bc3577c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6e81d-f0cd-4cdf-98fa-c8b827382277",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214e9f1-9e32-44fd-b8c3-9e588b3553db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikisql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79e7db5-5ee9-403c-9f3b-1b2d28d3973a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['phase', 'question', 'table', 'sql'],\n",
       "        num_rows: 15878\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['phase', 'question', 'table', 'sql'],\n",
       "        num_rows: 8421\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['phase', 'question', 'table', 'sql'],\n",
       "        num_rows: 56355\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9362660d-c873-4887-b11a-f4ae04dbbab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phase': 1,\n",
       " 'question': 'Tell me what the notes are for South Australia ',\n",
       " 'table': {'header': ['State/territory',\n",
       "   'Text/background colour',\n",
       "   'Format',\n",
       "   'Current slogan',\n",
       "   'Current series',\n",
       "   'Notes'],\n",
       "  'page_title': '',\n",
       "  'page_id': '',\n",
       "  'types': ['text', 'text', 'text', 'text', 'text', 'text'],\n",
       "  'id': '1-1000181-1',\n",
       "  'section_title': '',\n",
       "  'caption': '',\n",
       "  'rows': [['Australian Capital Territory',\n",
       "    'blue/white',\n",
       "    'Yaa·nna',\n",
       "    'ACT · CELEBRATION OF A CENTURY 2013',\n",
       "    'YIL·00A',\n",
       "    'Slogan screenprinted on plate'],\n",
       "   ['New South Wales',\n",
       "    'black/yellow',\n",
       "    'aa·nn·aa',\n",
       "    'NEW SOUTH WALES',\n",
       "    'BX·99·HI',\n",
       "    'No slogan on current series'],\n",
       "   ['New South Wales',\n",
       "    'black/white',\n",
       "    'aaa·nna',\n",
       "    'NSW',\n",
       "    'CPX·12A',\n",
       "    'Optional white slimline series'],\n",
       "   ['Northern Territory',\n",
       "    'ochre/white',\n",
       "    'Ca·nn·aa',\n",
       "    'NT · OUTBACK AUSTRALIA',\n",
       "    'CB·06·ZZ',\n",
       "    'New series began in June 2011'],\n",
       "   ['Queensland',\n",
       "    'maroon/white',\n",
       "    'nnn·aaa',\n",
       "    'QUEENSLAND · SUNSHINE STATE',\n",
       "    '999·TLG',\n",
       "    'Slogan embossed on plate'],\n",
       "   ['South Australia',\n",
       "    'black/white',\n",
       "    'Snnn·aaa',\n",
       "    'SOUTH AUSTRALIA',\n",
       "    'S000·AZD',\n",
       "    'No slogan on current series'],\n",
       "   ['Victoria',\n",
       "    'blue/white',\n",
       "    'aaa·nnn',\n",
       "    'VICTORIA - THE PLACE TO BE',\n",
       "    'ZZZ·562',\n",
       "    'Current series will be exhausted this year']],\n",
       "  'name': 'table_1000181_1'},\n",
       " 'sql': {'human_readable': 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA',\n",
       "  'sel': 5,\n",
       "  'agg': 0,\n",
       "  'conds': {'column_index': [3],\n",
       "   'operator_index': [0],\n",
       "   'condition': ['SOUTH AUSTRALIA']}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print example dataset\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56ba73d5-2bca-4f22-83e8-409133e36bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028b5c8c-4830-4c69-a624-f4936dfee7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_prefix = \"table:\"\n",
    "question_prefix = \"question:\"\n",
    "\n",
    "def preprocess_function(examples: Dict[str, Any]):\n",
    "    \"\"\"preprocess each row of wikisql datasets by create input with this format\n",
    "        {question_prefix} {natural_question} {table_prefix} {table_schema}\n",
    "        the labels will be the SQL statement\n",
    "        \n",
    "    Args:\n",
    "        examples (Dict[str, Any]): each row of datasets\n",
    "        \n",
    "    Returns:\n",
    "        output from tokenizer\n",
    "    \"\"\"\n",
    "    columns_merge = [\",\".join(table[\"header\"]) for table in examples[\"table\"]]\n",
    "    question_list = [question.replace(u'\\xa0', u' ') for question in examples[\"question\"]]\n",
    "    assert len(columns_merge) == len(question_list)\n",
    "    inputs = [f\"{question_prefix} {question_list[i]} {table_prefix} {columns_merge[i]}\" for i in range(len(columns_merge))]\n",
    "    targets = [sql[\"human_readable\"] for sql in examples[\"sql\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=512, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9c158-950e-44a6-a8cc-89c479d2e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run preprocess data\n",
    "train_dataset = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=[\"phase\", \"question\", \"table\", \"sql\"])\n",
    "test_dataset = dataset[\"test\"].map(preprocess_function, batched=True, remove_columns=[\"phase\", \"question\", \"table\", \"sql\"])\n",
    "val_dataset = dataset[\"validation\"].map(preprocess_function, batched=True, remove_columns=[\"phase\", \"question\", \"table\", \"sql\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207ff09-1c58-4f88-90f3-a43eaabd48fc",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ce01a82-0cb5-48a9-8cf1-c3996ab021f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da5fa751-43b1-4c39-9b4d-c0310e6bfaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cadb9ae-9474-4fb6-8750-ec641a1adff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=5e-6,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # gradient_checkpointing=True,\n",
    "    warmup_ratio=0.01,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,\n",
    "    predict_with_generate=True,\n",
    "    # generation_max_length=512,\n",
    "    # generation_num_beams=None,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # dataloader_num_workers=2,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7917f9d5-9831-4166-876d-974376bd6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     # Decode generated summaries into text\n",
    "#     decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "#     # Replace -100 in the labels as we can't decode them\n",
    "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "#     # Decode reference summaries into text\n",
    "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "#     # ROUGE expects a newline after each sentence\n",
    "#     decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "#     decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "#     # Compute ROUGE scores\n",
    "#     result = rouge_score.compute(\n",
    "#         predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "#     )\n",
    "#     # Extract the median scores\n",
    "#     result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "#     return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "990ef930-b1f3-4707-90ac-b1696d95da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    ") # you can evaluate by using compute_metrics function above, but I comment out for the faster training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b774a025-c79d-445c-9c45-30ce0fbab0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=\"./results/checkpoint-13000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48c9fda-2fb6-4338-af36-2a6f0c4fc123",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e78d95-f0af-4415-b355-48d800e92d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"./results/checkpoint-13000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "792d372b-9535-4666-a0b9-db3667c8bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "table_prefix = \"table:\"\n",
    "question_prefix = \"question:\"\n",
    "\n",
    "def prepare_input(question: str, table: List[str]):\n",
    "    print(\"question:\", question)\n",
    "    print(\"table:\", table)\n",
    "    join_table = \",\".join(table)\n",
    "    inputs = f\"{question_prefix} {question} {table_prefix} {join_table}\"\n",
    "    input_ids = tokenizer(inputs, max_length=700, return_tensors=\"pt\").input_ids\n",
    "    return input_ids\n",
    "\n",
    "def inference(question: str, table: List[str]) -> str:\n",
    "    input_data = prepare_input(question=question, table=table)\n",
    "    input_data = input_data.to(model.device)\n",
    "    outputs = model.generate(inputs=input_data, num_beams=10, top_k=10, max_length=512)\n",
    "    result = tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dee92f8-d4dc-4533-8222-32a5f201bee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Who is the director of the episode that corresponds to the total episodes number 14? \n",
      "table: ['Total#', 'Series#', 'Title', 'Writer', 'Director', 'Original air date']\n",
      "model result: SELECT Director FROM table WHERE Total# = 14\n",
      "real result: SELECT Director FROM table WHERE Total# = 14\n"
     ]
    }
   ],
   "source": [
    "test_id = 1000\n",
    "print(\"model result:\", inference(dataset[\"test\"][test_id][\"question\"], dataset[\"test\"][test_id][\"table\"][\"header\"]))\n",
    "print(\"real result:\", dataset[\"test\"][test_id][\"sql\"][\"human_readable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a139eafe-2d48-400f-8fae-36550bf3e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what is id with name jui and age equal 25\n",
      "table: ['id', 'name', 'age']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT id FROM table WHERE name = jui AND age = 25'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"what is id with name jui and age equal 25\", [\"id\",\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "914acb23-b3db-4e92-9503-e5c5f3910460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: get people name with age equal 25\n",
      "table: ['id', 'name', 'age']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT name FROM table WHERE age = 25'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(\"get people name with age equal 25\", [\"id\",\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b192edca-1108-40d3-962c-4409ad32ab20",
   "metadata": {},
   "source": [
    "## Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c111f5d-d236-4530-85ef-54372f017ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd1b96-2667-49c6-aa48-a42f939878c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(repo_id=\"juierror/flan-t5-text2sql-with-schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888562d-6083-45ec-bc05-f50096294b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(repo_id=\"juierror/flan-t5-text2sql-with-schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0522d-bfaa-4f9a-b7bc-e7a76033b2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
